Animation scheme
---
Anim:
	modeDefault
//	keys[]
	loopMode
		repeat
		pingpong
		once


Key:
	mode
		discrete - jump from one key to the next
		linear - lerp from one key to the next
		sine start/end/both - sine wave from one key to the next
		default (undefined) - uses default specified in Animation object

	time
		point in time of this key

	valueRef
		array of values (this way, can modify xform or colors of whatever length)
----






Server scheme
-----
AWS
	Route 53 to host the domain
	S3 to store static files (virtual server storage)
		Pay for storage USED, not PROVISIONED (unlike EC2, which is a dedicated server)
	Cloudfront as a CDN to connect domain requests to the S3 buckets
		Free tier: 1TB (1000GB) of Data Transfer Out and 10,000,000 HTTP/HTTPS requests each month
	DB:
		DynamoDB (NoSQL and document DB)
			"Not Only" SQL
			Basic key/value pair
			Not ideal for searching or filtering
			Simple DB, but have to build and maintain your own indexes for searching
			Can only store strings and numbers
			Not particularly efficient in bandwidth
			Could work for my needs
			Good for high request rate
				Like post data fetches?
		RDS
			Relational Database Service
			Overkill?
			Aurora
				$30/mo for a single-node config
				Storage through EBS volumes, auto-scaling available
					Can set up multi-region and backups
				Or Aurora Serverless as of 9/2018?
					Charge only for what you use, continuous scaling, high availability
					Can only be access from within a VPC... so maybe only from EC2?
					Can't access S3 (say, for bulk-loading data -- probably not a problem)
					Can't invoke Lambda functions from MySql functions(probably not a significant issue, but who knows)
					Can't connect via SSL (but can with user/pass) -- but still only within a VPC
					Storage rate: $0.10 per GB-month
					I/O rate: $0.20 per 1m requests
					Data CAN be transferred out to CloudFront, apparently! -- $0.00
						How does this work?  Would I be able to query the DB via the CDN?
					Not a huge diff per month from provisioned: ~$15/mo for 1 unit, $72 for 8
				MySQL 5.6 compatible, but up to 5x faster
				Managed, so resiliency, automatic failover, monitoring all handled
				Growth is handled without effort -- going viral is no problem
	Lambda
		Access point for AWS functions, such as DB interactions
		module: aws-sdk
	Cognito
		User authentication (ie, me, for admin pages)
	Electric Beanstalk
		Managing deploying scalable web applications
			Way overkill for this project
			Handles provisioning, load balancing, auto-scaling, health monitoring, zero-downtime deployments


Other tech
	Serverless (Yarn, NPM?)
		Tool for creating and deploying cloud functions, eg Amazon Lambda
		Lets me deploy frontend code automatically via plugins:
			serverless-s3-sync
			serverless-hooks
		Basically a script in VSCode to build dist and upload to AWS

Plan
	Route 53 domain
	Lambda->Cloudfront->S3
	Lambda->Cloudfront->Aurora Serverless v1
	Cognito for admin auth
----











First post!!!1!111

Ideas:
	Compose in Markdown
	Use runtime MD/HTML converter?
		Can I inject custom HTML this way?
		For <web-gl> and custom float/clear formatting...

Content:






# Basic shading: Diffuse and Specular

<web-gl width="400px" height="200px" src="assets/test/webgl1.json"></web-gl> 

##### What do diffuse and specular shading mean?  What is the difference?  How do they work?

Suggested prior reading:

- [Basic Geometry - Vertices, Indices, Triangles](posts/4)
- [Rasterization](posts/1)

-------

Displaying triangles on screen in any orientation we choose is all well and good, but they just looks like flat triangles unless you can convey the shape of the surface.  <web-gl width="400px" height="200px" src="assets/test/unshaded-box.json"></web-gl> 

There are two obvious ways to do this on a 2D screen: motion and shading.

If you choose motion, you can move either the camera (ie, the viewer's 'eye') or you can move the surface being viewed.  Motion is definitely cheaper than shading: you have to do the same amount of math to make objects show up on the screen whether their position or rotation changes each frame or not!  Here's what that looks like. <web-gl width="400px" height="200px" src="assets/test/unshaded-box-spin.json"></web-gl> 

Let's say you want to keep the camera where it is, or you have a lot of objects, or you want to convey shape without any motion.  The other option is one your eye has evolved to use reliably: shading.  Here's what that looks like.  <web-gl width="400px" height="200px" src="assets/test/shaded-box.json"></web-gl> 

If we're going to see to understand a shape, that means we have light.  And if we have light, we have a light source.  A light source means we have light traveling some direction to interact with the surface.  This will be some combination of reflect, absorb, or pass through, depending on the material -- imagine stainless steel, red brick, and paper, respectively.  The interaction of the light when it reaches the surface is the "shading" to which we refer.

You may have heard of "shaders" in the context of rendering and GPUs.  These refer to any specialized micro-program that runs on a GPU.  A math computation running on the GPU may use a single Compute Shader.  A single triangle can be drawn with just one Vertex Shader and one Fragment Shader (also known as a Pixel Shader).  Some advanced rendering applications use even more exotic versions as well, like Geometry Shaders or Tessellation Shaders.  All of these are simple programs that run exclusively on the GPU over and over again, once on each entry in a list: the only difference is on which list they run.

For more details on different kinds of shaders and how they work, see the post on [Shaders](posts/1).

The two types that do the kind of shading we're discussing are Vertex and Fragment Shaders.  Vertex Shaders run on a list of vertices (the preferred plural of vertex, though vertexes is acceptable), as you may have guessed, and are responsible for getting the vertices into the right place and facing the right direction.

What?  How can a point face a direction?

Well, each of those points comes with more data than just a position.  You may recall from the post on [Basic Geometry - Vertex, Index, Triangle] that a vertex typically has a normal, as well.  This normal is a unit vector (ie, a vector with a length of 1.0) that points "out" from the resulting triangle's face.

If every triangle had its own set of vertices with normals facing straight out from that triangle's face, we would see what we call "flat shading".  <web-gl width="400px" height="200px" src="assets/test/flat.json"></web-gl> 

If those triangles instead share a vertex (and hence the normal points in a direction that's an average of the triangles' facing), then a sort of smoothing takes place thanks to interpolation.

<web-gl width="400px" height="200px" src="assets/test/poscol-cube.json"></web-gl> 

Now we need to discuss interpolation.  When the data from 3 vertices is combined to become input for 1 fragment, the process to do that is interpolation.  The specific method is known as Gouraud interpolation [pronounced "GOO-row"](https://casual-effects.blogspot.com/2016/03/computational-graphics-pronunciation.html), described by Henri Gouraud in 1971.  It's actually very simple: to get a smooth value somewhere in the middle of a triangle, the GPU computes the weighted average of the values from the 3 vertices and passes it as an input to the Fragment Shader.  We can see this at work by showing the positions of a cube with one corner at (0,0,0) and the opposite at (1,1,1) as colors.

This is incredibly useful behavior by the GPU.  It's basically free math, per-fragment.  We don't have to pay Vertex Shader or Fragment Shader cycles for it -- it's just part of the pipeline that occurs no matter what.  In this way, offloading work from the Fragment Shader (which runs at least once for every pixel on screen) to the Vertex Shader (which typically runs hundreds or thousands of times less frequently) is usually a massive win for performance.  See the post on [Performance Basics](posts/2).

We have everything we need to display some basic lighting.  To keep things simple, we'll start with a directional light.  That's a light with only a direction -- no position.  It's considered to be infinitely far away with no falloff or cutoff distance, akin to sunlight but less physically accurate.  The good news is it's very easy to simulate.

All real-time lighting starts with a point in space, a normal, and an incoming light vector.  We have all of that.  The incoming light vector is just the opposite of the "sunlight" direction -- if the light is shining in the direction (1,0,0), then to me all the light is coming from the direction (-1,0,0).  That's the incoming light vector.  Note how it's also unit length?  That's important.

The fundamental calculation that we use throughout real-time lighting is [Lambert's cosine law](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law) (no relation).  Johann Heinrich Lambert, sometime around 1760, observed that the intensity of the light leaving an ideal surface is directly proportional to the cosine of the angle between the normal and the incoming light vector.  <svg width="40%" viewbox="0 0 100 56"><rect width="100%" height="100%" fill="white"/><rect x="25%" y="25%" width="50%" height="50%" fill="#ff0000" /><line x1="0" y1="0" x2="100%" y2="100%" style="stroke:rgb(0,0,255);stroke-width:2" /><rect x="0" y="0" width="100%" height="100%" style="fill-opacity:0;stroke:black;" stroke-dasharray="8" /></svg>

Footnote: this is also where I got my industry nickname.  See the [About](about) page.

Sorry for the math.  Thankfully, it ends up being pretty easy and the GPU does it all for us.  The cosine sounds expensive to calculate, and it is very expensive.  However, there's actually a nifty mathematical identity we can use, a sort of math shortcut.  See, the cosine of two unit-length vectors is identical to the result of the dot product between those two vectors, and that's extremely inexpensive to calculate.  You might recall from Geometry class that taking the cross product between two vectors gives you a vector, but taking a dot product between two vectors give you a single number (a scalar, we would say in graphics land).  This is the most sacred use of the dot product: it has applications throughout gaming, including in audio, collisions, physics -- anytime you want to calculate the rebound value of something off a surface.

In fact, this one use here, in the Vertex Shader, combined with Gouraud interpolation by the GPU, gives us basic lighting.  We take the scalar from the dot product and multiply it against the light color, if there is one, and send the result as a color value to the Fragment Shader.  The Fragment Shader simply shows the interpolated result.  This is known as Diffuse vertex lighting.  <web-gl width="400px" height="200px" src="assets/test/diff-vs.json"></web-gl> 

Neat, but it's also very flat.  You can click and drag the camera around the object to see what I mean.  The lighting doesn't change relative to your viewpoint -- that's a hallmark of "Diffuse" lighting.  The next part of the lighting stew adds that view-dependent component, which we call "Specular" lighting.

Specular lighting is calculated in almost the exact same way as Diffuse lighting, except with two more steps and one more bit of info: the direction to the viewer.  

Imagine a wet road with the sun opposite you.  You would expect to see a reflection of the sun, a hot spot, on the ground, some distance between you and the sun.  We can calculate this hotspot with the Diffuse lighting algorithm technique -- we just need to plug in different vectors for the Lambert cosine calculation.  Let's work backwards.  If you're calculating the lighting at the point of the hotspot, one of them would be the normal of the ground, obviously.  The other is going to be "dotted" against the normal to get the lighting strength coefficient for this hotspot.  If we use the vector towards the sun, then we'd just get a view-independent Diffuse lighting value -- no view-relative hotspot at all.  If we use the vector towards the eye, then we'd get a result that never involved the sun at all.  But maybe something in between?  Literally?  That's actually exactly what we use: the average of the vector to the eye and the vector to the light.  We call it the "half vector".  Note that after you average them, the result probably won't be of length 1 anymore, so we have to normalize it again, which involves a square root operation (to get the length of the vector) and thus isn't super cheap.

But with that done, we have a normal and a half vector at the point of our hotspot.  If we now apply Lambert's Cosine Law, we get a scalar result that tells us how powerful the sun's hotspot should be at that point.  And as we move, or as the sun moves, that value will change.  Let's see our Specular vertex lighting in action, in addition to the Diffuse lighting from before.  <web-gl width="400px" height="200px" src="assets/test/spec-diff-vs.json"></web-gl> 

You can see the shape for sure, but you can also see the how it's interpolating between vertices.  As the light source moves around, you can really see the artifacts of Gouraud interpolation on a low-polygon object show up.  You can get around this with more triangles, or by moving the calculation to run per-fragment in a Fragment Shader instead.

Moving work from the Vertex Shader to the Fragment Shader is actually pretty simple.  We just have to make sure the Vertex Shader outputs anything the Fragment Shader needs to do the work, and that the Fragment Shader has access to any constant data (see [GPU Memory](posts/3)).  In this case, the Vertex Shader outputs the world-space position and world-space normal to the Fragment Shader by assigning them to specially marked variables that were registered as part of the declaration of these shaders.  The GPU will dutifully interpolate these vectors and the Fragment Shader takes the results, performing the same calculations with the same constants.  The results, however, are much nicer: they should be, since instead of running on a few dozen vertices, they're running for thousands of pixels!  <web-gl width="400px" height="200px" src="assets/test/spec-diff-fs.json"></web-gl> 

Note the fps and frame duration differences between the Vertex Shader and Fragment Shader Specular examples.  While for any given render the Vertex Shader expense scales with vertex count, Fragment Shader expense scales with size on screen and screen resolution.  Essentially, the more pixels that are eventually covered by an object, then the more times the GPU will have to run the Fragment Shader.  See the [Performance](posts/2) post for more about this.

-- Shader 









// This could be part of a [geometry] post
If one was to draw a triangle, each point would need a position.  So that's the first step.  But with only a position, we know nothing more about the "shape" of the surface the point is helping to form -- only the location.  Sure, we know there's a surface running through that point in space -- what in what direction does the surface face at that point?  There's a way to say, actually: we can give each point a "normal" vector.  The normal vector is a unit vector, which just means it's of length 1.0, and it points "outwards".  So on the surface of the earth, all normal vectors on all the vertices forming the surface would seem to point straight up into space.

Now we have two data elements for each vertex: a position vector, and a normal vector.

But what if you don't want the adjacent triangles to blend smoothly into each other?  For a sphere, sure, that's desirable.  But for a cube, you want hard edges!  That's actually easy to solve: you just use extra vertices at the corners.  Instead of 1 vertex at each corner, you have 3: one for each face direction.  All three vertices at a corner will have the same exact position, but a different normal.  You make sure the triangles for each face use the correct vertices for that face, and then the fragment shader will process the pixels for the triangles it needs.

For more information, see the post on [Rasterization].
// 















// OPEN A POPUP WINDOW
		// eslint-disable-next-line max-len
		const editor = window.open( 'about:blank', 'Editor', 'scrollbars=no,resizeable=yes,status=no,location=no,toolbar=no,menubar=no,width=1280,height=720,top=200,left=200' );
		if ( editor === null )
			throw new Error( 'Failed to open new Editor popup window' );

		const contents = `
<html>
<head>
	<style>
		html,
		body {
			padding: 0px;
			margin: 0px;
			background-color: #cfcfcf;
			min-height: 100vh;
			font-family: sans-serif;
		}

		.navContent {
			max-width: 1140;
		}

		.top {
			display:flex;
			flex-direction:column;
			height: 95%;
		}

		.admin-container {
			display: flex;
			gap: 20px;
			flex-grow: 1;
		}

		.edit-panel {
			width: 100%;
		}

		.preview-panel {
			width: 100%;
			flex-grow: 1;
			background-color: #efefef;
		}

		#editPostTextBox {
			width: 100%;
			height: 100%;
			font-size: 13px;
		}

		#previewPostBox {
			width: 100%;
			height: 100%;
			font-size: 15px;
		}
	</style>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/showdown/2.1.0/showdown.min.js"></script>
</head>
<body>
<div class="top">
	<div class="admin-container">
		<div class="edit-panel">
			<textarea id="editPostTextBox"> </textarea>
		</div>
		<div class="preview-panel">
			<div id="previewPostBox"> </div>
		</div>
	</div>
</div>
</body>
</html>
		`;

		editor.document.write( contents );

		const textArea = editor.document.getElementById( 'editPostTextBox' );
		if ( textArea === null )
			throw new Error( 'Couldn\'t find text area DOM element' );

		const previewArea = editor.document.getElementById( 'previewPostBox' );
		if ( previewArea === null )
			throw new Error( 'Couldn\'t find preview area DOM element' );

		textArea.textContent = '';

		textArea.oninput = ( ev =>
		{
			const event = ev.target as HTMLInputElement;
			previewArea.innerHTML = convertMDtoHTML( event.value );
		} );




























Working example (note space and double newline after </web-gl>)
----------------
<web-gl src="assets/test/poscol-cube.json"></web-gl> 

Now we need to discuss interpolation.  When the data from 3 vertices is combined to become input for 1 fragment, the process to do that is interpolation.  The specific method is known as Gouraud interpolation [pronounced "GOO-row"](https://casual-effects.blogspot.com/2016/03/computational-graphics-pronunciation.html), described by Henri Gouraud in 1971.  It's actually very simple: to get a smooth value somewhere in the middle of a triangle, the GPU computes the weighted average of the values from the 3 vertices and passes it as an input to the Fragment Shader.  We can see this at work by showing the positions of a cube with one corner at (0,0,0) and the opposite at (1,1,1) as colors.

<web-gl src="assets/test/poscol-cube.json"></web-gl> 

Now we need to discuss interpolation.  When the data from 3 vertices is combined to become input for 1 fragment, the process to do that is interpolation.  The specific method is known as Gouraud interpolation [pronounced "GOO-row"](https://casual-effects.blogspot.com/2016/03/computational-graphics-pronunciation.html), described by Henri Gouraud in 1971.  It's actually very simple: to get a smooth value somewhere in the middle of a triangle, the GPU computes the weighted average of the values from the 3 vertices and passes it as an input to the Fragment Shader.  We can see this at work by showing the positions of a cube with one corner at (0,0,0) and the opposite at (1,1,1) as colors.
----------------




Font options
--
Catamaran
* Inter
Lato
Merriweather Sans light
Merriweather Sans
Montserrat
Open Sans
Roboto

Twind defaults:

sans: 'ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"'.split(','),

serif: 'ui-serif,Georgia,Cambria,"Times New Roman",Times,serif'.split(','),

mono: 'ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace'.split(',')





WebGL rendering goal
	1 render loop for all WebGL elements (if any)
	Only loop if any animated
	If any animated, start animation loop
	If none animated, cancel animation loop
	If scroll or resize event, request new render only if not animated

WebGL element caching
	Values:
		key: divID 'web-gl-container-1'
		validator: outerHTML '<web-gl class="webglembed" fontsize="24" src="assets/test/webgl2.json"></web-gl>'
		value: object
	Current:
		textarea: onInput
			webgl.onNavigateAway to clear viewports and reset WebGL
		doConversion: (cleanup )
	New:
		Don't reset WebGL
		Don't blindly create new viewports:
			Instead, submit a divID and outerHTML and see if the key and outerHTML matches an existing viewport.  
				If they do, return the existing viewport.
				If not, destroy the existing viewport and create a new one for that divID, returning it.
			If none exists, return a new one.
		How to clear unused divIDs?  Maybe a manual cleanup step?  Or just the final onNavigateAway cleanup?





DB access plan
POSTS
	Get all posts, sorted by latest publish date (or the client could do the sort...)
	Get a post of name 'blah'
	Get all posts with tag 'category'
	Get all threads, sorted by latest publish date (or client sorts)
	Get a thread of name 'blah'
EDITOR
	Get all posts, sorted by created date (or publish is fine, so long as unpublished is first, or client sorts)
	Get a post of name 'blah'
	Put a post of name 'blah' with data 'blob'

Post
*	name 			- partition key
*	title
	author
*	dateCreated
*	dateModified?
*	datePublished	- sort key? -- update alongside "dateModified" until first published
*	tags
	hdrInline
	hdrHref
	hdrAlt
*	description
	body
Thread
*	name			- secondary key?
*	title
*	description
	posts[]
The single-table version of this has 1 Post Item per post Partition, but potentially many Thread Items per post Partition, using a tid GSI



Amazon DynamoDB Deep Dive: Advanced Design Patterns for DynamoDB (AWS re:Invent 2018) talk
	Relational databases born in the 70s because storage was expensive
		Normalizing, reducing data footprint, was extremely important
		But increases CPU cost to denormalize data so the CPU could consume it
	Now, the CPU is the most expensive resource, so why optimize for storage instead?
		NoSQL is the answer to this problem
	NoSQL
		Optimized for compute
		Instantiated views instead of ad-hoc queries
		Build for transaction processing at scale, not analytic processing
	DynamoDB
		Fully Managed NoSQL
		Document or Key-Value
			Wide-column key-value store that supports a document attribute type
		Scales to Any Workload
		Fast and Consistent
			Single tables running at 4m transactions per second
			Low single-digit ms latency
			The busier it gets, the more consistent the low-latency responses become
				(because of caching)
		Access Control
			Can restrict access to the table itself, to the items within the table (different query patterns?)
		Event Driven Programming
		Scales horizontally
	Table is more like a catalog of items (instead of rows)
	Items don't always have to have the same attributes
		But they always have to have a "Partition Key"
			The Partition Key uniquely identifies the item
		Optional "Sort Key"
			Gives the ability to execute complex range queries against the items in those partitions
			So a partition is like a folder or bucket and the sort key orders the items within that folder
			Rich query capabilities:
				All items for key
				==, <, >, >=, <=
				"begins with"
				"between"
				"contains"
				"in"
				sorted results
				counts
				top/botton N values
			EG:
				partition key is customer ID
				sort key is order date
				primary access pattern is all customer orders in last 24 hours
				Query: partition key = "CustomerID-X" and the sort key is > 24 hours ago
	Partition Keys
		Uniquely ID the item
		Distribute items across the key space
		Used to hash the items across the key space
			Routes to correct storage node for that item
	Sort Keys
		Uses two attributes together (Partition:Sort Key) to uniquely ID an Item
		Within unordered hash index, data is arranged by the sort key
		No limit on the number of items (ie, infinite) per partition key
			Except if you have local secondary indexes (??)
	So: Partitions are buckets that contain Items, like rows, that share a Partition ID
		The SortKey uniquely identifies an Item (row) inside that Partition.
		You could have a Partition per customer, then a SortKey for order #, then have dozens of order Items per customer Sorted inside each customer's Partition
	Eventually Consistent vs Strongly Consistent
		All writes go to 3 replicas
		At first, there's a primary write and in less than 1ms the first replica is committed (so 2 copies)
		The third write will take longer (depending on load?)
		EC reads randomly choose from one of the 3
		SC reads always read from primary, and thus are always up-to-date, but cost twice as much
	Local Secondary Index (LSI)
		LSIs limit the # of range keys to a 10GB max per partition key
		Alternate Sort Key attribute
		Index is local to a Partition Key
		Allow you to "re-sort" the data in the partitions
		Let's you use a different access pattern (sort)
		However, must ALWAYS USE the same Partition Key as the original access pattern
	Global Secondary Index (GSI)
		Alternate Partition and/or Sort Keys
		Index is across all Partition Keys
		Use composite Sort Keys for compound indexes
		RCUs/WCUs are provisioned separately for GSIs
		EG:
			instead of grouping by customer, I could group by warehouse:
				PK is warehouse ID and SK is order date
	How do GSI Updates work?
		GSI Updates are Eventually Consistent
		LSIs are Strongly Consistent
		Need to make sure GSIs have enough capacity to handle the update rate from the table if writing table data frequently
	Max DynamoDB Efficiency
		"To get the most out of DynamoDB throughput, create tables where the partition key element has a large number of distinct values, and values are requested fairly uniformly, as randomly as possible."
	Answering complex questions
		DynamoDB Streams and Lambda
		Totally separated from the DB, so very fault-tolerant
		Stream is the changelog for the DynamoDB table
			All write ops appear on the Stream
		Once the data is on the Stream you can invoke a Lambda function
		That Lambda function has two IAM roles
			Invocation Role
				defines what it can see/read
				which items can it see, which attributes on items can it read
			Execution Role
				defines what it can do
				what other AWS services on your account does it have access to
				what permissions does it have on those services to work with this data
		Example uses:
			Computed aggregations: counts, sums
				You can keep a running count/sum, write it back into the table
				Always available to read for basically free
			Push data into Kinesis firehose for stream processing
			Interact with external systems
			Could even skip Lambda and use an EC2 instance to read the stream constantly if the rate is high enough
	Query filters
		DynamoDB gives you 2 key conditions you can evaluate in a query
		The first applies to the sort key (the sort condition)
			This limits the number of items you read, because it's on the sort key (probably?)
		The second is the filter condition
			You still pay to read these items, because it's on a non-special key (probably?)
	Composite keys
		To avoid the cost of the read and accomplish a similar thing as a filter, that is, filter using the sort:
			Concatenante two keys into a single string
			Instead of "sort by date, filter by PENDING" (to get 2 out of 10,000 items)
				Create StatusDate key, concatenating status and date (eg, PENDING_2022-10-13)
				Query starts with "PENDING_" to get a status/date range search, and sort as well, limiting read to the minimum required
				(this shows how important it is to know your access patterns and tune your data)
	DynamoDB Transactions API
		Update multiple items at the same time
		


This resulted in an MD HR:
"
How does one draw a goddamn HR around here 

---

---
*__SERIOUSLY FUCKED__* 
"
It was a dashed line though.
{
	height: 0px;
	color: inherit;
	border-top-width: 1px;
}