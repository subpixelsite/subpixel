Animation scheme
---
Anim:
	modeDefault
//	keys[]
	loopMode
		repeat
		pingpong
		once


Key:
	mode
		discrete - jump from one key to the next
		linear - lerp from one key to the next
		sine start/end/both - sine wave from one key to the next
		default (undefined) - uses default specified in Animation object

	time
		point in time of this key

	valueRef
		array of values (this way, can modify xform or colors of whatever length)
----






Server scheme
-----
AWS
	Route 53 to host the domain
	S3 to store static files (virtual server storage)
		Pay for storage USED, not PROVISIONED (unlike EC2, which is a dedicated server)
	Cloudfront as a CDN to connect domain requests to the S3 buckets
		Free tier: 1TB (1000GB) of Data Transfer Out and 10,000,000 HTTP/HTTPS requests each month
	DB:
		DynamoDB (NoSQL and document DB)
			"Not Only" SQL
			Basic key/value pair
			Not ideal for searching or filtering
			Simple DB, but have to build and maintain your own indexes for searching
			Can only store strings and numbers
			Not particularly efficient in bandwidth
			Could work for my needs
			Good for high request rate
				Like post data fetches?
		RDS
			Relational Database Service
			Overkill?
			Aurora
				$30/mo for a single-node config
				Storage through EBS volumes, auto-scaling available
					Can set up multi-region and backups
				Or Aurora Serverless as of 9/2018?
					Charge only for what you use, continuous scaling, high availability
					Can only be access from within a VPC... so maybe only from EC2?
					Can't access S3 (say, for bulk-loading data -- probably not a problem)
					Can't invoke Lambda functions from MySql functions(probably not a significant issue, but who knows)
					Can't connect via SSL (but can with user/pass) -- but still only within a VPC
					Storage rate: $0.10 per GB-month
					I/O rate: $0.20 per 1m requests
					Data CAN be transferred out to CloudFront, apparently! -- $0.00
						How does this work?  Would I be able to query the DB via the CDN?
					Not a huge diff per month from provisioned: ~$15/mo for 1 unit, $72 for 8
				MySQL 5.6 compatible, but up to 5x faster
				Managed, so resiliency, automatic failover, monitoring all handled
				Growth is handled without effort -- going viral is no problem
	Lambda
		Access point for AWS functions, such as DB interactions
		module: aws-sdk
	Cognito
		User authentication (ie, me, for admin pages)
	Electric Beanstalk
		Managing deploying scalable web applications
			Way overkill for this project
			Handles provisioning, load balancing, auto-scaling, health monitoring, zero-downtime deployments


Other tech
	Serverless (Yarn, NPM?)
		Tool for creating and deploying cloud functions, eg Amazon Lambda
		Lets me deploy frontend code automatically via plugins:
			serverless-s3-sync
			serverless-hooks
		Basically a script in VSCode to build dist and upload to AWS

Plan
	Route 53 domain
	Lambda->Cloudfront->S3
	Lambda->Cloudfront->Aurora Serverless v1
	Cognito for admin auth
----











First post!!!1!111

Ideas:
	Compose in Markdown
	Use runtime MD/HTML converter?
		Can I inject custom HTML this way?
		For <web-gl> and custom float/clear formatting...

Content:



# Basic shading: Diffuse and Specular

<web-gl href="shaded-box.json"/>

##### What do diffuse and specular shading mean?  What is the difference?  How do they work?

Suggested prior reading:

- [Basic Geometry - Vertices, Indices, Triangles](posts/4)
- [Rasterization](posts/1)

-------

Displaying triangles on screen in any orientation we choose is all well and good, but they just looks like flat triangles unless you can convey the shape of the surface.  <web-gl href="unshaded-box.json"/>

There are two obvious ways to do this on a 2D screen: motion and shading.

If you choose motion, you can move either the camera (ie, the viewer's 'eye') or you can move the surface being viewed.  Motion is definitely cheaper than shading: you have to do the same amount of math to make objects show up on the screen whether their position or rotation changes each frame or not!  Here's what that looks like. <web-gl href="unshaded-box-spin.json"/>

Let's say you want to keep the camera where it is, or you have a lot of objects, or you want to convey shape without any motion.  The other option is one your eye has evolved to use reliably: shading.  Here's what that looks like.  <web-gl href="shaded-box.json"/>

If we're going to see to understand a shape, that means we have light.  And if we have light, we have a light source.  A light source means we have light traveling some direction to interact with the surface.  This will be some combination of reflect, absorb, or pass through, depending on the material -- imagine stainless steel, red brick, and paper, respectively.  The interaction of the light when it reaches the surface is the "shading" to which we refer.

You may have heard of "shaders" in the context of rendering and GPUs.  These refer to any specialized micro-program that runs on a GPU.  A math computation running on the GPU may use a single Compute Shader.  A single triangle can be drawn with just one Vertex Shader and one Fragment Shader (also known as a Pixel Shader).  Some advanced rendering applications use even more exotic versions as well, like Geometry Shaders or Tessellation Shaders.  All of these are simple programs that run exclusively on the GPU over and over again, once on each entry in a list: the only difference is on which list they run.

For more details on different kinds of shaders and how they work, see the post on [Shaders](posts/1).

The two types that do the kind of shading we're discussing are Vertex and Fragment Shaders.  Vertex Shaders run on a list of vertices (the preferred plural of vertex, though vertexes is acceptable), as you may have guessed, and are responsible for getting the vertices into the right place and facing the right direction.

What?  How can a point face a direction?

Well, each of those points comes with more data than just a position.  You may recall from the post on [Basic Geometry - Vertex, Index, Triangle] that a vertex typically has a normal, as well.  This normal is a unit vector (ie, a vector with a length of 1.0) that points "out" from the resulting triangle's face.

If every triangle had its own set of vertices with normals facing straight out from that triangle's face, we would see what we call "flat shading".  <web-gl href="flat.json"/>

If those triangles instead share a vertex (and hence the normal points in a direction that's an average of the triangles' facing), then a sort of smoothing takes place thanks to interpolation.

Now we need to discuss interpolation.  When the data from 3 vertices is combined to become input for 1 fragment, the process to do that is interpolation.  The specific method is known as Gouraud interpolation [pronounced "GOO-row"](https://casual-effects.blogspot.com/2016/03/computational-graphics-pronunciation.html), described by Henri Gouraud in 1971.  It's actually very simple: to get a smooth value somewhere in the middle of a triangle, the GPU computes the weighted average of the values from the 3 vertices and passes it as an input to the Fragment Shader.  We can see this at work by showing the positions of a cube with one corner at (0,0,0) and the opposite at (1,1,1) as colors.  <web-gl href="poscol-cube.json"/>

This is incredibly useful behavior by the GPU.  It's basically free math, per-fragment.  We don't have to pay Vertex Shader or Fragment Shader cycles for it -- it's just part of the pipeline that occurs no matter what.  In this way, offloading work from the Fragment Shader (which runs at least once for every pixel on screen) to the Vertex Shader (which typically runs hundreds or thousands of times less frequently) is usually a massive win for performance.  See the post on [Performance Basics](posts/2).

We have everything we need to display some basic lighting.  To keep things simple, we'll start with a directional light.  That's a light with only a direction -- no position.  It's considered to be infinitely far away with no falloff or cutoff distance, akin to sunlight but less physically accurate.  The good news is it's very easy to simulate.

All real-time lighting starts with a point in space, a normal, and an incoming light vector.  We have all of that.  The incoming light vector is just the opposite of the "sunlight" direction -- if the light is shining in the direction (1,0,0), then to me all the light is coming from the direction (-1,0,0).  That's the incoming light vector.  Note how it's also unit length?  That's important.

The fundamental calculation that we use throughout real-time lighting is [Lambert's cosine law](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law) (no relation).  Johann Heinrich Lambert, sometime around 1760, observed that the intensity of the light leaving an ideal surface is directly proportional to the cosine of the angle between the normal and the incoming light vector.  [interactive svg or webgl with vector and cosine value and brightness?]

Footnote: this is also where I got my industry nickname.  See the [About](about) page.

Sorry for the math.  Thankfully, it ends up being pretty easy and the GPU does it all for us.  The cosine sounds expensive to calculate, and it is very expensive.  However, there's actually a nifty mathematical identity we can use, a sort of math shortcut.  See, the cosine of two unit-length vectors is identical to the result of the dot product between those two vectors, and that's extremely inexpensive to calculate.  You might recall from Geometry class that taking the cross product between two vectors gives you a vector, but taking a dot product between two vectors give you a single number (a scalar, we would say in graphics land).  This is the most sacred use of the dot product: it has applications throughout gaming, including in audio, collisions, physics -- anytime you want to calculate the rebound value of something off a surface.

In fact, this one use here, in the Vertex Shader, combined with Gouraud interpolation by the GPU, gives us basic lighting.  We take the scalar from the dot product and multiply it against the light color, if there is one, and send the result as a color value to the Fragment Shader.  The Fragment Shader simply shows the interpolated result.  This is known as Diffuse vertex lighting.  <web-gl href="diffuse-vs.json"/>

Neat, but it's also very flat.  You can click and drag the camera around the object to see what I mean.  The lighting doesn't change relative to your viewpoint -- that's a hallmark of "Diffuse" lighting.  The next part of the lighting stew adds that view-dependent component, which we call "Specular" lighting.

Specular lighting is calculated in almost the exact same way as Diffuse lighting, except with two more steps and one more bit of info: the direction to the viewer.  

Imagine a wet road with the sun opposite you.  You would expect to see a reflection of the sun, a hot spot, on the ground, some distance between you and the sun.  We can calculate this hotspot with the Diffuse lighting algorithm technique -- we just need to plug in different vectors for the Lambert cosine calculation.  Let's work backwards.  If you're calculating the lighting at the point of the hotspot, one of them would be the normal of the ground, obviously.  The other is going to be "dotted" against the normal to get the lighting strength coefficient for this hotspot.  If we use the vector towards the sun, then we'd just get a view-independent Diffuse lighting value -- no view-relative hotspot at all.  If we use the vector towards the eye, then we'd get a result that never involved the sun at all.  But maybe something in between?  Literally?  That's actually exactly what we use: the average of the vector to the eye and the vector to the light.  We call it the "half vector".  Note that after you average them, the result probably won't be of length 1 anymore, so we have to normalize it again, which involves a square root operation (to get the length of the vector) and thus isn't super cheap.

But with that done, we have a normal and a half vector at the point of our hotspot.  If we now apply Lambert's Cosine Law, we get a scalar result that tells us how powerful the sun's hotspot should be at that point.  And as we move, or as the sun moves, that value will change.  Let's see our Specular vertex lighting in action, in addition to the Diffuse lighting from before.  <web-gl href="spec-diff-vs.json"/>

You can see the shape for sure, but you can also see the how it's interpolating between vertices.  As the light source moves around, you can really see the artifacts of Gouraud interpolation on a low-polygon object show up.  You can get around this with more triangles, or by moving the calculation to run per-fragment in a Fragment Shader instead.

Moving work from the Vertex Shader to the Fragment Shader is actually pretty simple.  We just have to make sure the Vertex Shader outputs anything the Fragment Shader needs to do the work, and that the Fragment Shader has access to any constant data (see [GPU Memory](posts/3)).  In this case, the Vertex Shader outputs the world-space position and world-space normal to the Fragment Shader by assigning them to specially marked variables that were registered as part of the declaration of these shaders.  The GPU will dutifully interpolate these vectors and the Fragment Shader takes the results, performing the same calculations with the same constants.  The results, however, are much nicer: they should be, since instead of running on a few dozen vertices, they're running for thousands of pixels!  <web-gl href="spec-diff-fs.json"/>

Note the fps and frame duration differences between the Vertex Shader and Fragment Shader Specular examples.  While for any given render the Vertex Shader expense scales with vertex count, Fragment Shader expense scales with size on screen and screen resolution.  Essentially, the more pixels that are eventually covered by an object, then the more times the GPU will have to run the Fragment Shader.  See the [Performance](posts/2) post for more about this.

-- Shader




// This could be part of a [geometry] post
If one was to draw a triangle, each point would need a position.  So that's the first step.  But with only a position, we know nothing more about the "shape" of the surface the point is helping to form -- only the location.  Sure, we know there's a surface running through that point in space -- what in what direction does the surface face at that point?  There's a way to say, actually: we can give each point a "normal" vector.  The normal vector is a unit vector, which just means it's of length 1.0, and it points "outwards".  So on the surface of the earth, all normal vectors on all the vertices forming the surface would seem to point straight up into space.

Now we have two data elements for each vertex: a position vector, and a normal vector.

But what if you don't want the adjacent triangles to blend smoothly into each other?  For a sphere, sure, that's desirable.  But for a cube, you want hard edges!  That's actually easy to solve: you just use extra vertices at the corners.  Instead of 1 vertex at each corner, you have 3: one for each face direction.  All three vertices at a corner will have the same exact position, but a different normal.  You make sure the triangles for each face use the correct vertices for that face, and then the fragment shader will process the pixels for the triangles it needs.

For more information, see the post on [Rasterization].
// 



